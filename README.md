# Customer Churn Prediction

## ğŸ“Œ Description
This project is part of the **Udacity MLOps Engineer Nanodegree â€“ February 2025**.

The goal is to **refactor** the existing notebook (`src/notebooks/churn_notebook.ipynb`) by applying **clean code best practices** and improving modularity.  
For implementation details, refer to `src/notebooks/Guide.jpynb`.

### ğŸ”¹ **Key Features**
âœ” **PEP 8 compliant & well-documented**  
âœ” **Modular, maintainable, and reusable ML pipeline**  
âœ” **Flexible configuration via `config/config.json`**  
âœ” **Stores EDA plots, trained models, and evaluation metrics**  
âœ” **Efficient handling of categorical variables**  

---

## ğŸ“‚ **Project Structure**
## Project Structure

ğŸ“ **customer_churn/** *(Main project directory)*
â”‚â”€â”€ ğŸ“„ **README.md** *(Project documentation)*
â”‚â”€â”€ ğŸ“„ **requirements.txt** *(Dependencies)*
â”‚â”€â”€ ğŸ“ **config/** *(Configuration files)*
â”‚   â”œâ”€â”€ ğŸ“„ **config.json** *(Main configuration file)*
â”‚   â”œâ”€â”€ ğŸ“„ **preprocessing_config.json**
â”‚   â”œâ”€â”€ ğŸ“„ **data_splitting_profiles.json**
â”‚   â”œâ”€â”€ ğŸ“„ **training_config.json**
â”‚â”€â”€ ğŸ“ **data/** *(Dataset storage)*
â”‚   â”œâ”€â”€ ğŸ“ **raw/** *(Original data)*
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ **bank_data.csv**
â”‚   â”œâ”€â”€ ğŸ“ **processed/** *(Preprocessed data)*
â”‚â”€â”€ ğŸ“ **logs/** *(Logging output)*
â”‚   â”œâ”€â”€ ğŸ“„ **customer_churn.log**
â”‚â”€â”€ ğŸ“ **models/** *(Trained models)*
â”‚   â”œâ”€â”€ ğŸ“„ **LogisticRegression.pkl**
â”‚   â”œâ”€â”€ ğŸ“„ **RandomForestClassifier.pkl**
â”‚â”€â”€ ğŸ“ **results/** *(Evaluation results)*
â”‚   â”œâ”€â”€ ğŸ“ **images/** *(Plots and visualizations)*
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ **roc_curve.png**
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ **shap_RandomForestClassifier.png**
â”‚   â”‚   â”œâ”€â”€ ğŸ“ **eda/** *(Exploratory Data Analysis)*
â”‚   â”‚       â”œâ”€â”€ ğŸ–¼ï¸ **bar_chart_marital_status.png**
â”‚   â”‚       â”œâ”€â”€ ğŸ–¼ï¸ **correlation_heatmap.png**
â”‚   â”œâ”€â”€ ğŸ“ **json/** *(Evaluation metrics)*
â”‚       â”œâ”€â”€ ğŸ“„ **evaluation.json**
â”‚â”€â”€ ğŸ“ **src/** *(Source code)*
â”‚   â”œâ”€â”€ ğŸ **churn_library.py** *(Main ML pipeline)*
â”‚   â”œâ”€â”€ ğŸ **config_manager.py** *(Configuration handler)*
â”‚   â”œâ”€â”€ ğŸ **logger_config.py** *(Logging setup)*
â”‚   â”œâ”€â”€ ğŸ“ **common/** *(Shared utilities)*
â”‚   â”‚   â”œâ”€â”€ ğŸ **exceptions.py** *(Custom exceptions)*
â”‚   â”œâ”€â”€ ğŸ“ **data_preprocessing/** *(Data cleaning & encoding)*
â”‚   â”‚   â”œâ”€â”€ ğŸ **data_cleaner.py**
â”‚   â”‚   â”œâ”€â”€ ğŸ **data_encoder.py**
â”‚   â”œâ”€â”€ ğŸ“ **eda/** *(Exploratory Data Analysis)*
â”‚   â”‚   â”œâ”€â”€ ğŸ **eda_visualizer.py**
â”‚   â”œâ”€â”€ ğŸ“ **models/** *(Model training & evaluation)*
â”‚   â”‚   â”œâ”€â”€ ğŸ **data_splitter.py**
â”‚   â”‚   â”œâ”€â”€ ğŸ **model_trainer.py**
â”‚   â”‚   â”œâ”€â”€ ğŸ **model_evaluator.py**
â”‚   â”œâ”€â”€ ğŸ“ **notebooks/** *(Jupyter notebooks, provided as reference)*
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ **Guide.ipynb**
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ **churn_notebook.ipynb**
â”‚â”€â”€ ğŸ“ **tests/** *(Unit tests using pytest)*
â”‚   â”œâ”€â”€ ğŸ **test_data_cleaner.py**
â”‚   â”œâ”€â”€ ğŸ **test_data_encoder.py**
â”‚   â”œâ”€â”€ ğŸ **test_data_splitter.py**
â”‚   â”œâ”€â”€ ğŸ **test_data_visualizer.py**
â”‚   â”œâ”€â”€ ğŸ **test_model_trainer.py**
â”‚   â”œâ”€â”€ ğŸ **test_model_evaluator.py**
â”‚â”€â”€ ğŸ“ **utils/** *(Utility scripts)*
â”‚   â”œâ”€â”€ ğŸ **pylint_checker.py** *(automatically generates a pylint report for all source code)*


---

## âš™ **Installation**
### ğŸ‘… **1. Clone the repository**
```bash
git clone https://github.com/Marcennaji/customer_churn.git
cd customer_churn
```
### ğŸ“¦ **2. Install dependencies**
```bash
pip install -r requirements.txt
```

---

## ğŸš€ **Usage**
### **1. Configure the pipeline**
- Open `config/config.json` in the root directory  
- Set the `"data_path"` to your dataset location  
- To enable **evaluation-only mode**, set:
  ```json
  "eval_only": true
  ```

### **2. Run the ML pipeline**
```bash
python src/churn_library.py --config=config/config.json
```

---

## ğŸ›  **Running Tests**
### **Run all unit tests**
```bash
pytest
```
### **Run tests with verbose output**
```bash
pytest -v
```
- All test files are located in the `tests/` directory  
- Test configuration is managed via `pytest.ini`  

---

## ğŸ“ **Logging & Debugging**
### **Check Logs**
```bash
cat logs/customer_churn.log
```
- Logs include **both info and error messages**
- Stored in `customer_churn.log` after script execution.

---

## ğŸ” **Code formatting and linting**

To ensure high-quality and maintainable code, this project follows strict formatting and linting guidelines.

### Formatting

Instead of `autopep8` and `pylint`, this project uses `black` and `ruff` for code formatting and linting, as configured in `.pre-commit-config.yaml`. The reasons for this choice:

- **black**: Provides consistent, opinionated formatting, ensuring uniformity across all scripts.
- **ruff**: A fast Python linter and formatter, which also provides additional checks and optimizations beyond `black`.

#### autopep8 vs. black

Both autopep8 and black format Python code, but:
autopep8: Focuses on fixing PEP 8 violations (less opinionated).
black: Enforces a strict, opinionated style (makes all code look uniform).
black is the better choice for standardization and team projects.

#### pylint vs. ruff

Both pylint and ruff check for code style and quality, but:
pylint: More thorough but slow and sometimes overly strict.
ruff: Faster, supports many pylint rules, and can auto-fix some issues.
ruff is a modern alternative to pylint, often preferred for speed.

However, for those who want a pylint report, a dedicated script, `pylint_checker.py`, is provided to analyze all Python files recursively and generate both a summary and a detailed report.
Simply run:

```bash
python utils/pylint_checker.py
```
---

## ğŸ“Š **Stored Images & Models**
### âœ… **EDA Plots (Saved in `results/images/eda`)**
âœ” **Univariate (quantitative)** â†’ `histogram_age.png`  
âœ” **Univariate (categorical)** â†’ `bar_chart_marital_status.png`  
âœ” **Bivariate Plot** â†’ `correlation_heatmap.png`

### âœ… **Evaluation Plots (Saved in `results/images/`)**
âœ” **ROC Curves** â†’ `roc_curve.png`  
âœ” **Feature Importances** â†’ `feature_importance_RandomForestClassifier.png`  
âœ” **SHAP Explanation** â†’ `shap_RandomForestClassifier.png`

### âœ… **Stored Models (`models/` Directory)**
âœ” **Logistic Regression (`LogisticRegression.pkl`)**  
âœ” **Random Forest (`RandomForestClassifier.pkl`)**  

Models are stored in **.pkl format using Joblib** for easy deployment.

---

## ğŸ”¬ **Handling Categorical Variables**
- Uses **mean encoding** or **one-hot encoding** based on configuration  
- Efficiently processes categorical columns via **looping**  
- Defined in `src/data_preprocessing/data_encoder.py`

---

## ğŸ›  **Future Improvements**
âœ” **Optimize hyperparameter tuning strategy**  
âœ” **Enhance model versioning with MLflow**  
âœ” **Add Docker support for easy deployment**  

---

## ğŸ‘¨â€ğŸ’¼ **Author**
**Marc Ennaji**  
ğŸŒ [GitHub](https://github.com/Marcennaji) | ğŸ“ [LinkedIn](https://linkedin.com/in/marcennaji)  

---

## ğŸ **License**
This project is licensed under the **MIT License**.  

