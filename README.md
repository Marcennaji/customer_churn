# Customer Churn Prediction (Work in Progress)

## Description
Udacity MLOps Engineer Nanodegree â€“ February 2025

This project aim to refactor the src/notebooks/churn_notebook.ipynb file, by applying clean code best practices. See src/notebooks/Guide.jpynb for details.

The proposed solution aim to be flexible, generic, maintenable and easily configurable for different purposes, by providing a set of ML classes that goes beyond the specific problematic of a churn detection.

## Project Architecture
```
	ğŸ“ . Main project
	  ğŸ“„ .pre-commit-config.yaml 
	  ğŸ“„ README.md 
	  ğŸ“ config Directory containing configuration files
	    ğŸ“„ data_splitting_profiles.json 
	    ğŸ“„ preprocessing_config.json 
	    ğŸ“„ training_config.json 
	  ğŸ“„ config.json 
	  ğŸ“ data Directory containing data files
	    ğŸ“ processed 
	    ğŸ“ raw 
	      ğŸ“„ bank_data.csv 
	  ğŸ“ images Directory containing image files
	  ğŸ“ logs 
	    ğŸ“„ customer_churn.log 
	  ğŸ“ models 
	    ğŸ“„ LogisticRegression.pkl 
	    ğŸ“„ RandomForestClassifier.pkl 
	  ğŸ“„ pytest.ini 
	  ğŸ“„ requirements.txt 
	  ğŸ“ results Results produced by the ML pipeline execution
	    ğŸ“ images 
	      ğŸ“ eda 
	        ğŸ–¼ï¸ bar_chart_marital_status.png 
	        ğŸ–¼ï¸ correlation_heatmap.png 
	        ğŸ–¼ï¸ histogram_age.png 
	        ğŸ–¼ï¸ histogram_churn.png 
	        ğŸ–¼ï¸ kde_total_transaction_count.png 
	      ğŸ–¼ï¸ feature_importance_RandomForestClassifier.png 
	      ğŸ–¼ï¸ roc_curve.png 
	      ğŸ–¼ï¸ shap_RandomForestClassifier.png 
	    ğŸ“ json 
	      ğŸ“„ evaluation.json 
	  ğŸ“ src Source code directory
	    ğŸ churn_library.py _Module for loading and evaluating models - This module serves as the main pipeline for the customer churn project, handling data processing, model training, and evaluation._
	    ğŸ“ common 
	      ğŸ exceptions.py _This module defines custom exceptions for the customer churn project._
	    ğŸ config_manager.py _Configuration file manager module - _
	    ğŸ“ data_preprocessing 
	      ğŸ data_cleaner.py _This module handles general data cleaning operations for the customer churn project._
	      ğŸ data_encoder.py _This module handles categorical feature encoding based on JSON configuration for the customer churn project._
	      ğŸ encoder_type.py _This module provides an abstract base class for categorical encoders in the customer churn project._
	    ğŸ“ eda Exploratory Data Analysis module
	      ğŸ eda_visualizer.py _This module handles Exploratory Data Analysis (EDA) visualizations for the customer churn project._
	    ğŸ logger_config.py 
	    ğŸ“ models Model training and evaluation module
	      ğŸ data_splitter.py _This module handles train-test data splitting based on JSON configuration profiles for the customer churn project._
	      ğŸ model_evaluator.py _This module handles model evaluation, visualization, and feature importance reporting for the customer churn project._
	      ğŸ model_trainer.py _This module handles the training and hyperparameter tuning of models for the customer churn project._
	    ğŸ“ notebooks Provided notebooks for reference
	      ğŸ“„ Guide.ipynb 
	      ğŸ“„ churn_notebook.ipynb 
	      ğŸ churn_notebook.py 
	    ğŸ“ results 
	      ğŸ“ images 
	        ğŸ“ eda 
	          ğŸ–¼ï¸ bar_chart_marital_status.png 
	          ğŸ–¼ï¸ correlation_heatmap.png 
	          ğŸ–¼ï¸ histogram_age.png 
	          ğŸ–¼ï¸ histogram_churn.png 
	          ğŸ–¼ï¸ kde_total_transaction_count.png 
	  ğŸ“„ test.md 
	  ğŸ“ tests 
	    ğŸ test_data_cleaner.py 
	    ğŸ test_data_encoder.py 
	    ğŸ test_data_splitter.py 
	    ğŸ test_eda_visualizer.py 
	    ğŸ test_model_evaluator.py 
	    ğŸ test_model_trainer.py 
	  ğŸ“ utils Utility functions directory
	    ğŸ generate_doc_tree.py 
	    ğŸ“„ pylint_checker.out 
	    ğŸ pylint_checker.py _Script for checking Python code quality - _


```

## Prerequisites
- Python 3.10+
- Libraries: see `requirements.txt`

## Installation
Clone the repository and install dependencies:
```bash
git clone https://github.com/Marcennaji/customer_churn.git
cd customer_churn
pip install -r requirements.txt
pip install -e .
```

## Usage
See the python script `churn_library.py`, for an example of a complete ML pipeline.
For executing the ML pipeline on a sample dataset with default parameters values, run:
```bash
churn_library --csv=data/raw/bank_data.csv
```
You can also override parameters default values, for example:
```bash
churn_library --preprocessing-config=config/preprocessing_config.json --splitting-config=config/data_splitting_profiles.json --training-config=config/training_config.json  --csv=data/raw/bank_data.csv --data-dir=data --models-dir=models
```
## Tests
Run unit tests:
```bash
pytest
```

## Author
**Marc Ennaji** 

## License
This project is licensed under the MIT License.

